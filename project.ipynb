{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import BaggingRegressor, ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv('fall_project_dataset/development.csv', index_col=0)\n",
    "df_eval = pd.read_csv('fall_project_dataset/evaluation.csv', index_col=0)\n",
    "\n",
    "df = pd.concat([df_dev, df_eval])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the cardinality of OCCP, POBP, MIGSP, SCHL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the cardinality of the OCCP column 529 -> 25\n",
    "# Create a dictionary from the OCCP code to the text representation\n",
    "reader = csv.reader(open('produced_documents/occp_to_string.csv', 'r'), delimiter=';')\n",
    "next(reader, None) # Skip the headers\n",
    "\n",
    "occp_to_string = {}\n",
    "\n",
    "for row in reader:\n",
    "   k, v = row\n",
    "   k = int(k)\n",
    "   occp_to_string[k] = v\n",
    "\n",
    "# Map the OCCP column to its text values\n",
    "df['OCCP'] = df['OCCP'].map(occp_to_string)\n",
    "\n",
    "# Keep only the first 3 characters \n",
    "df['OCCP'] = df['OCCP'].apply(lambda occp : occp[0:3])\n",
    "\n",
    "# Group the countries of POBP column by continent\n",
    "# 219 -> 6\n",
    "df['POBP'] = pd.cut(df['POBP'], bins=[0,1,100,200,300,400,500], right=False, labels=['N/A', 'USA', 'Europe', 'Asia', 'Americas', 'Oceania'], include_lowest=True)\n",
    "\n",
    "# Group the countries of MIGSP column by continent\n",
    "# 96 -> 6\n",
    "df['MIGSP'] = pd.cut(df['MIGSP'], bins=[0,1,100,200,300,400,500], right=False, labels=['N/A', 'USA', 'Europe', 'Asia', 'Americas', 'Oceania'], include_lowest=True)\n",
    "\n",
    "# Group fine grained education categories together\n",
    "# 24 -> 10\n",
    "df['SCHL'] = pd.cut(df['SCHL'], bins=[0,1,8,10,15,19,20,21,22,23,24], right=True, labels=['No', 'Low', 'Primary', 'Junior High', 'High', 'Associate', 'Bachelor', 'Master', 'Professional', 'PhD'], include_lowest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the minutes range from JWAP and JWDP and calculate the possible range of JWMNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = csv.reader(open('produced_documents/JWDP.csv', 'r'))\n",
    "next(reader, None) # Skip the headers\n",
    "\n",
    "jwdp_begin = {}\n",
    "jwdp_end = {}\n",
    "\n",
    "for row in reader:\n",
    "   k, b, e = row\n",
    "   k = int(k)\n",
    "   jwdp_begin[k] = int(b)\n",
    "   jwdp_end[k] = int(e)\n",
    "\n",
    "reader = csv.reader(open('produced_documents/JWAP.csv', 'r'))\n",
    "next(reader, None) # Skip the headers\n",
    "\n",
    "jwap_begin = {}\n",
    "jwap_end = {}\n",
    "\n",
    "for row in reader:\n",
    "   k, b, e = row\n",
    "   k = int(k)\n",
    "   jwap_begin[k] = int(b)\n",
    "   jwap_end[k] = int(e)\n",
    "\n",
    "# Map the JWDP column to extract minimum and maximum departure time in minutes\n",
    "df['JWDP_B'] = df['JWDP'].map(jwdp_begin)\n",
    "df['JWDP_E'] = df['JWDP'].map(jwdp_end)\n",
    "\n",
    "# Map the JWAP column to extract minimum and maximum arrival time in minutes\n",
    "df['JWAP_B'] = df['JWAP'].map(jwap_begin)\n",
    "df['JWAP_E'] = df['JWAP'].map(jwap_end)\n",
    "\n",
    "# Add two columns for the expected JWMNP range\n",
    "df['JWMNP_B'] = df['JWAP_B'] - df['JWDP_B']\n",
    "df['JWMNP_E'] = df['JWAP_E'] - df['JWDP_E']\n",
    "df['JWMNP_B_E'] = df['JWAP_E'] - df['JWDP_B']\n",
    "df['JWMNP_E_B'] = df['JWAP_B'] - df['JWDP_E']\n",
    "df['JWMNP_A'] = (df['JWAP_E'] - df['JWDP_B']) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop high correlation columns and all the JWDP, JWAP columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['MIG', 'PAOC', 'FER', 'VPS', 'JWDP', 'JWAP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep only the 5 most frequent values in the LANP column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5 = df['LANP'].value_counts().head(5).index\n",
    "\n",
    "# In case the language is not in the list the assigned value is NaN\n",
    "# 121 -> 5\n",
    "df['LANP'] = df['LANP'].where(df['LANP'].isin(top5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features One-hot encoding and numeric features scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['WKHP', 'PINCP', 'JWMNP_B', 'JWMNP_E', 'JWMNP_B_E', 'JWMNP_E_B', 'JWMNP_A']\n",
    "categorical_features = ['JWDP_B', 'JWDP_E', 'JWAP_B', 'JWAP_E', 'COW', 'SCHL', 'MAR', 'OCCP', 'POBP', 'SEX', 'RAC1P', 'HICOV', 'LANP', 'PUBCOV', 'DEAR', 'MIGSP', 'ENG', 'OC', 'FDEYEP', 'MIL']\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore', drop=\"if_binary\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide df_dev and df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = df[df[\"JWMNP\"].notna()]\n",
    "df_eval = df[df[\"JWMNP\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection\n",
    "## Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_dev.drop(columns=[\"JWMNP\"])\n",
    "y = df_dev[\"JWMNP\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9958614922789507"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('regressor', BaggingRegressor())])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9956511683176013"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('regressor', ExtraTreesRegressor(n_jobs=-1))])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9967005619038779"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('regressor', GradientBoostingRegressor())])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9962243687800295"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('regressor', RandomForestRegressor(n_jobs=-1))])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning\n",
    "## GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.996983)\n",
      "Best parameters: {'regressor__criterion': 'friedman_mse', 'regressor__learning_rate': 0.2, 'regressor__max_depth': 6, 'regressor__n_estimators': 75, 'regressor__random_state': 42}\n",
      "Best estimator score on test data: 0.997107\n"
     ]
    }
   ],
   "source": [
    "gbr_pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('regressor', GradientBoostingRegressor())])\n",
    "\n",
    "gbr_param_grid={\n",
    "    'regressor__learning_rate': [0.1, 0.01, 0.2],\n",
    "    'regressor__n_estimators': [50, 75, 100, 200],\n",
    "    'regressor__criterion': ['friedman_mse', 'squared_error'],\n",
    "    'regressor__max_depth': [3, 4, 5, 6],\n",
    "    'regressor__random_state': [42]\n",
    "}\n",
    "\n",
    "gbr_search = GridSearchCV(gbr_pipe, gbr_param_grid, scoring=\"r2\", n_jobs=-1)\n",
    "\n",
    "gbr_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameter (CV score=%f)\" % gbr_search.best_score_)\n",
    "print(f\"Best parameters: {gbr_search.best_params_}\")\n",
    "print(\"Best estimator score on test data: %f\" % gbr_search.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameter (CV score=0.996995)\n",
    "Best parameters: {'regressor__learning_rate': 0.2, 'regressor__max_depth': 6, 'regressor__n_estimators': 75, 'regressor__random_state': 42}\n",
    "Best estimator score on test data: 0.997113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9970935020767074"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('regressor', GradientBoostingRegressor(learning_rate=0.2, n_estimators=75, criterion=\"friedman_mse\", max_depth=6, random_state=42))])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('regressor', GradientBoostingRegressor(learning_rate=0.2, n_estimators=75, criterion=\"friedman_mse\", max_depth=6, random_state=42))])\n",
    "\n",
    "pipe.fit(X, y)\n",
    "\n",
    "y_pred = pipe.predict(df_eval)\n",
    "\n",
    "data = list(zip(df_eval.index, y_pred))\n",
    "\n",
    "# Save the data to a CSV file\n",
    "with open('submissions/submission_x.csv', 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(['Id', 'Predicted'])  # Header row\n",
    "    csvwriter.writerows(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
